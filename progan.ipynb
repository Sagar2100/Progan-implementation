{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2177371,"sourceType":"datasetVersion","datasetId":1307206}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing modules\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport os\nfrom keras.optimizers import Adam\nfrom keras.layers import Input, Dense, Flatten, Reshape, LeakyReLU, UpSampling2D, AveragePooling2D, Layer, Add \nfrom keras.initializers import RandomNormal\nfrom keras.models import Model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Batch_size=16\nEPSILON=1e-8\nLR = 1e-3\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classes use later\nclass PixelNorm(Layer):\n    def __init__(self, **kwargs):\n        super(PixelNorm, self).__init__(**kwargs)\n    \n    def call(self, inputs):\n        values = inputs**2.0\n        mean_value = keras.backend.mean(values, axis=1, keepdims=True)\n        mean_value += EPSILON\n        l2 = keras.backend.sqrt(mean_value)\n        normalized = inputs/l2\n        return normalized\n    \n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n\n\nclass Minibatchstdev(Layer):\n    def __init__(self, **kwargs):\n        super(Minibatchstdev, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        mean=keras.backend.mean(inputs, axis=0, keepdims=True)\n        sq_diff = keras.backend.square(inputs-mean)\n        mean_sq_diff = keras.backend.mean(sq_diff, axis=0, keepdims=True)\n        mean_sq_diff += EPSILON\n        stdev = keras.backend.sqrt(mean_sq_diff)\n\n        mean_pix = keras.backend.mean(stdev, keepdims=True)\n        shape = keras.backend.shape(inputs)\n        output= keras.backend.tile(mean_pix, (shape[0], shape[1], shape[2], 1))\n\n        combined = keras.backend.concatenate([inputs, output], axis=-1)\n        return combined\n    \n    def compute_output_shape(self, input_shape):\n        input_shape = list(input_shape)\n        input_shape[-1] += 1\n        return tuple(input_shape)\n\n\nclass WeighedSum(Add):\n    def __init__(self, alpha=0.0, **kwargs):\n        super(WeighedSum, self).__init__(**kwargs)\n        self.alpha = keras.backend.variable(alpha, name=\"ws_Alpha\")\n    \n    def _merge_function(self, inputs):\n        assert (len(inputs)==2)\n        # ((1-a)*input1) + (a*input2)\n        output = ((1.0- self.alpha)*inputs[0]) + (self.alpha * inputs[1])\n        return output\n    \n\n\ndef update_fadein(models, step, n_steps):\n    alpha= min(step/float(n_steps-1), 1)\n    for model in models:\n        for layer in model.layers:\n            if isinstance(layer, WeighedSum):\n                keras.backend.set_value(layer.alpha, alpha)\n    return alpha\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EQUALIZED LEARNING RATE\nclass EqualizedConv(Layer):\n    def __init__(self, out_channels, kernal, gain=2, padding=\"valid\", **kwargs):\n        super().__init__(**kwargs)\n        self.kernal = kernal\n        self.out_channels= out_channels\n        self.gain = gain\n        self.pad = kernal != 1\n        self.padding = padding.upper()\n\n    def build(self, input_shape):\n        self.in_channels = input_shape[-1]\n        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n        self.w = self.add_weight(\n            shape=[self. kernal,self.kernal, self.in_channels, self.out_channels],\n            initializer=initializer,\n            trainable=True,\n            name=\"kernal\",\n        )\n        self.b = self.add_weight(\n            shape=(self.out_channels,),\n            initializer=\"zeros\",\n            trainable=True,\n            name=\"bias\"\n        )\n\n        fan_in = self.kernal*self.kernal*self.in_channels\n        self.scale = tf.sqrt(self.gain/fan_in)\n\n    \n    def call(self, inputs):\n        x= inputs\n        output=(tf.nn.conv2d(x, self.scale*self.w, strides=1, padding=self.padding)+self.b)\n        return output\n\n\nclass EqualizedConvT(Layer):\n    def __init__(self, out_channels, kernal, gain=2, padding=\"valid\", **kwargs):\n        super().__init__(**kwargs)\n        self.kernal = kernal\n        self.out_channels= out_channels\n        self.gain = gain\n        self.pad = kernal != 1\n        self.padding = padding.upper()\n\n    def build(self, input_shape):\n        self.in_channels = input_shape[-1]\n        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n        self.w = self.add_weight(\n            shape=[self. kernal,self.kernal, self.in_channels, self.out_channels],\n            initializer=initializer,\n            trainable=True,\n            name=\"kernal\",\n        )\n        self.b = self.add_weight(\n            shape=(self.out_channels,),\n            initializer=\"zeros\",\n            trainable=True,\n            name=\"bias\"\n        )\n\n        fan_in = self.kernal*self.kernal*self.in_channels\n        self.scale = tf.sqrt(self.gain/fan_in)\n\n    \n    def call(self, inputs):\n        x= inputs\n        os=[tf.shape(x)[0], 4,4,self.out_channels]\n        output=(\n            tf.nn.conv2d_transpose(x, self.scale*self.w, strides=1,output_shape=os, padding=self.padding)+self.b)\n        return output\n\n\n\nclass EqualizedDense(Layer):\n    def __init__(self, units ,gain=2, **kwargs):\n        super().__init__(**kwargs)\n        self.units= units\n        self.gain = gain\n\n    def build(self, input_shape):\n        self.in_channels = input_shape[-1]\n        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n        self.w = self.add_weight(\n            shape=[self.in_channels, self.units],\n            initializer=initializer,\n            trainable=True,\n            name=\"kernal\",\n        )\n        self.b = self.add_weight(\n            shape=(self.units,),\n            initializer=\"zeros\",\n            trainable=True,\n            name=\"bias\"\n        )\n\n        fan_in = self.in_channels\n        self.scale = tf.sqrt(self.gain/fan_in)\n\n    \n    def call(self, inputs):\n        output=tf.add(tf.matmul(inputs, self.scale*self.w),self.b)\n        return output\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define generator model\ndef define_generator(latent_dim):\n\n    in_latent = Input(shape=(latent_dim,))\n    g = Reshape((1,1,512))(in_latent)\n    g = PixelNorm()(g)\n\n    # con 4x4 block\n    g = EqualizedConvT(512, kernal=4, padding=\"valid\")(g)\n    g = LeakyReLU(alpha=0.2)(g)\n    g = PixelNorm()(g)\n\n    # con 3x3 block\n    g = EqualizedConv(512, kernal=3, padding=\"same\")(g)\n    g = LeakyReLU(alpha=0.2)(g)\n    g = PixelNorm()(g)\n\n    # con 1x1 output block\n    out_img = EqualizedConv(3, kernal=1, padding=\"same\")(g)\n    model = Model(inputs=[in_latent], outputs=out_img)\n    return model\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define discriminator model\ndef define_discriminator(input_shape=(4,4,3)):\n    in_img = Input(shape=input_shape)\n\n    d = EqualizedConv(512, kernal=1, padding=\"same\")(in_img)\n    d = LeakyReLU(alpha=0.2)(d)\n\n    d = Minibatchstdev()(d)\n\n    d = EqualizedConv(512, kernal=3, padding=\"same\")(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    d = EqualizedConv(512, kernal=4, padding=\"valid\")(d)\n    d = LeakyReLU(alpha=0.2)(d)\n\n    d = Flatten()(d)\n    out_class = EqualizedDense(1)(d)\n\n    model = Model(inputs=[in_img], outputs=out_class)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Addin generator block\ndef add_generator_block(old_model, filters):\n    block_end = old_model.layers[-2].output\n    \n    upsampling = UpSampling2D()(block_end)\n    g = EqualizedConv(filters, kernal=3, padding=\"same\")(upsampling)\n    g = LeakyReLU(alpha=0.2)(g)\n    g = PixelNorm()(g)\n    g = EqualizedConv(filters, kernal=3, padding=\"same\")(upsampling)\n    g = LeakyReLU(alpha=0.2)(g)\n    g = PixelNorm()(g)\n\n    out_img = EqualizedConv(3, kernal=1, padding=\"same\")(g)\n    model1 = Model(inputs=old_model.input, outputs=out_img)\n    out_old = old_model.layers[-1]\n    out_img2 = out_old(upsampling)\n\n    merged = WeighedSum()([out_img2, out_img])\n    model2 = Model(old_model.input, merged)\n\n    return [model1, model2]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add discriminator block\ndef add_discrominator_block(old_model, filter1, filter2):\n    in_shape = list(old_model.input_shape)\n\n    input_shape = (in_shape[-2]*2, in_shape[-2]*2, in_shape[-1])\n    in_img = Input(shape=input_shape)\n\n    d = EqualizedConv(filter1, kernal=3, padding=\"same\")(in_img)\n    d = LeakyReLU(alpha=0.2)(d)\n\n    d = EqualizedConv(filter1, kernal=3, padding=\"same\")(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    d = EqualizedConv(filter2, kernal=3, padding=\"same\")(d)\n    d = LeakyReLU(alpha=0.2)(d)\n    d = AveragePooling2D((2,2))(d)\n    block_new = d\n\n    # skipp input 1x1 and activation of old model\n    for i in range(3, len(old_model.layers)):\n        d = old_model.layers[i](d)\n    \n    model1 = Model(in_img, d)\n\n    downsample = AveragePooling2D((2,2))(in_img)\n    block_old = old_model.layers[1](downsample)\n    block_old = old_model.layers[2](block_old)\n\n    d = WeighedSum()([block_old, block_new])\n\n    for i in range(3, len(old_model.layers)):\n        d = old_model.layers[i](d)\n    \n    model2 = Model(in_img, d)\n    return [model1, model2]\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_blocks(gmodel, dmodel, filterg, filterd):\n    gmodels = add_generator_block(gmodel, filterg)\n    dmodels = add_discrominator_block(dmodel, filterd[0], filterd[1])\n\n    return gmodels, dmodels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GAN(keras.Model):\n    def __init__(self, disc_fade, disc_direct, gen_fade, gen_direct, latent_dim):\n        super().__init__()\n        self.discrminator = disc_fade\n        self.generator = gen_fade\n        self.gen_direct = gen_direct\n        self.disc_direct = disc_direct\n        self.latent_dim = latent_dim\n        self.seed = tf.random.normal([9, latent_dim])\n        self.d_loss_tracker = keras.metrics.Mean(name=\"D_loss\")\n        self.g_loss_tracker = keras.metrics.Mean(name=\"G_loss\")\n        self.alpha_tracker = keras.metrics.Mean(name=\"Alpha\")\n        self.alpha=0.0\n\n    def compile(self,g_opt, d_opt):\n        super().compile()\n        self.d_optimizer = d_opt\n        self.g_optimizer = g_opt\n\n    \n    def get_models(self):\n        return [self.gen_direct, self.disc_direct]\n    \n    def update_alpha(self, new_alp):\n        self.alpha_tracker.reset_state()\n        self.alpha_tracker.update_state(new_alp)\n    \n    def generator_loss(self, fake_img):\n        return -tf.reduce_mean(fake_img)\n    \n    def discriminator_loss(self, real_img, fake_img, epsilion_drift=0.001):\n        real_loss=tf.reduce_mean(real_img)\n        fake_loss=tf.reduce_mean(fake_img)\n        drift_loss = epsilion_drift*tf.reduce_mean(tf.square(real_img))\n        return fake_loss-real_loss + drift_loss\n    \n    def gradient_penalty(self, real, fake, disc):\n        alpha=tf.random.uniform([self.Batch_size, 1,1,1], 0.0, 1.0)\n        diff = fake-real\n        interpolated= real + alpha*diff\n        \n        with tf.GradientTape() as gp_tape:\n            gp_tape.watch(interpolated)\n            pred = disc(interpolated, training=True)\n\n        grads = gp_tape.gradient(pred, [interpolated])[0]\n        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1,2,3]))\n        gp = tf.reduce_mean((norm-1.0)**2)\n        return gp\n    \n\n    def show_samples(self, epoch, res, s=True):\n        predictions = self.gen_direct(self.seed, training=False)\n        predictions = (predictions+1)/2\n\n        fig = plt.figure(figsize=(5,5))\n\n        for i in range(9):\n            plt.subplot(3,3,i+1)\n            plt.imshow(predictions[i])\n            plt.axis(\"off\")\n\n        if s:\n            plt.savefig(f\"Images/image_at_{res}_at_epoch_{epoch}.png\")\n        plt.show()\n\n    def train_step(self, real_images):\n        if isinstance(real_images, tuple):\n            real_images= real_images[0]\n        \n        self.Batch_size=tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal([self.Batch_size, self.latent_dim])\n\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n            gen_img = self.generator(random_latent_vectors, training=True)\n\n            real_out=self.discrminator(real_images, training=True)\n            fake_out=self.discrminator(gen_img, training=True)\n\n            gen_loss=self.generator_loss(fake_out)\n            d_cost = self.discriminator_loss(real_out, fake_out)\n            gp = self.gradient_penalty(real_images, gen_img, self.discrminator)\n            disc_loss = d_cost + 10*gp\n\n            gradients_of_genrator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n            self.g_optimizer.apply_gradients(zip(gradients_of_genrator, self.generator.trainable_variables))\n\n            gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discrminator.trainable_variables)\n            self.d_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discrminator.trainable_variables))\n\n        # update metrics\n        self.d_loss_tracker.reset_state()\n        self.g_loss_tracker.reset_state()\n        self.d_loss_tracker.update_state(disc_loss)\n        self.g_loss_tracker.update_state(gen_loss)\n\n        return {\n            \"d_loss\":self.d_loss_tracker.result(),\n            \"g_loss\":self.g_loss_tracker.result(),\n            \"alpha\":self.alpha_tracker.result(),\n        }\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython import display\nclass CustomCallback(keras.callbacks.Callback):\n    def __init__(self, step, n_step, cr, ckmanager):\n        super().__init__()\n        self.n_step=n_step\n        self.step = step\n        self.ckpt_manager = ckmanager\n        self.res=cr\n    \n    def on_epoch_end(self, epoch, logs=None):\n        display.clear_output(wait=True)\n        self.model.show_samples(epoch, self.res)\n        self.ckpt_manager.save()\n        print(\"Model Saved\")\n    \n    def on_train_batch_begin(self, batch, logs=None):\n        self.step=self.step+1\n        self.alp=update_fadein([self.model.generator, self.model.discrminator],self.step, self.n_step)\n        self.model.update_alpha(self.alp)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path=\"/kaggle/input/celebahq-resized-256x256/celeba_hq_256/*\"\nres=16\n\ndef load(image_path):\n    img=tf.io.read_file(image_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [res,res])\n    img = tf.cast(img, tf.float32)\n    img= (img-127.5)/127.5\n    return img\n\ndataset=tf.data.Dataset.list_files(path).map(load).batch(Batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=80\nn_step=(epochs/2)*len(dataset)\nprint(len(dataset))\nprint(n_step)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/device:GPU:0\"):\n    g_model=define_generator(512)\n    d_model = define_discriminator()\n    gan=GAN(disc_fade=d_model, disc_direct=d_model, gen_fade=g_model, gen_direct=g_model, latent_dim=512)\n\nD_optimizer = Adam(learning_rate=LR, beta_1=0.0, beta_2=0.99, epsilon=EPSILON)\nG_optimizer = Adam(learning_rate=LR, beta_1=0.0, beta_2=0.99, epsilon=EPSILON)\ngan.compile(G_optimizer, D_optimizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Upsampler\nwith tf.device(\"/device:GPU:0\"):\n    g_model, d_model= gan.get_models()\n    gmodels, dmodels = add_blocks(g_model, d_model, 512, [512,512])\n    gan=GAN(disc_fade=dmodels[1], disc_direct=dmodels[0], gen_fade=gmodels[1], gen_direct=gmodels[0], latent_dim=512)\n\nD_optimizer = Adam(learning_rate=LR, beta_1=0.0, beta_2=0.99, epsilon=EPSILON)\nG_optimizer = Adam(learning_rate=LR, beta_1=0.0, beta_2=0.99, epsilon=EPSILON)\ngan.compile(G_optimizer, D_optimizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check Points\ncheckpoint_path=f\"./checkpoints/Models/RES_{res}\"\nckpt=tf.train.Checkpoint(generator=gmodels[1],\n                         discriminator=dmodels[1],\n                         generator_direct=gmodels[0],\n                         discriminator_direct=dmodels[0], )\n\nckpt_manager=tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# restore checkpoint\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print(\"checkpoint restored !\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir(\"Images\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gan.fit(dataset, epochs=epochs, batch_size=Batch_size, callbacks=[CustomCallback(0, n_step,res, ckpt_manager)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gan.show_samples(5,5,s=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show sample from dataset\niteer=next(dataset.take(1).as_numpy_iterator())\nplt.imshow(iteer[0])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample genrated by model\nnoise=tf.random.normal([1, 512])\nimg=g_model(noise, training=False)\nimg=(img[0]+1)/2\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}